{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext as sc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession, functions, types\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import math, re, urllib, requests\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = sc(appName=\"attraction\")\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.appName('attraction analysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_path = 'outputs/attraction_details'\n",
    "rev_path = 'outputs/attraction_reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_df = spark.read.parquet(det_path)\n",
    "det_df.createOrReplaceTempView('det_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(det_df.count())\n",
    "display(det_df.orderBy('attraction_id').toPandas().head(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_loc_udf = functions.udf(lambda x: re.sub('things_to_do_in_','',x),StringType())\n",
    "det_df = det_df.withColumn('city',clean_loc_udf(det_df.city))\n",
    "det_df = det_df.withColumn('country',clean_loc_udf(det_df.country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_nonull = det_df.where(det_df.rating.isNull()).withColumn(\"rating\", functions.lit(-1))\n",
    "out_df = det_df.where(det_df.rating.isNotNull()).union(rat_nonull)\n",
    "display(out_df.count())\n",
    "out_df.createOrReplaceTempView('out_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myround(x, base=.5):\n",
    "    return float(round(x/base)*base)\n",
    "\n",
    "avg_rat_df = spark.sql(\"SELECT province, category, AVG(rating) as avg_rating FROM out_df WHERE rating != -1 GROUP BY province, category\")\n",
    "round_udf = functions.udf(lambda x: myround(x), FloatType())\n",
    "avg_rat_df = avg_rat_df.withColumn('updated_rating',round_udf(avg_rat_df.avg_rating)).drop('avg_rating')\n",
    "\n",
    "out_df = out_df.join(avg_rat_df, ['province','category'],'left').orderBy('attraction_id')\n",
    "out_df = out_df.withColumn(\"rating\", functions.when(out_df[\"rating\"]== -1, out_df[\"updated_rating\"]).otherwise(out_df[\"rating\"])).drop('updated_rating')\n",
    "\n",
    "out_df.createOrReplaceTempView('out_df')\n",
    "display(out_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_price_df = spark.sql(\"SELECT province, category, AVG(price) as avg_price FROM out_df WHERE price > 0 GROUP BY province, category\")\n",
    "round_price_udf = functions.udf(lambda x: round(x,2), FloatType())\n",
    "avg_price_df = avg_price_df.withColumn('updated_price', round_price_udf(avg_price_df[\"avg_price\"]))\n",
    "\n",
    "upd_price_df = out_df.join(avg_price_df, ['province','category'],'left_outer').orderBy('attraction_id')\n",
    "det_df = upd_price_df.withColumn(\"price\", functions.when(det_df[\"price\"] == -1.00, upd_price_df[\"updated_price\"]).otherwise(upd_price_df[\"price\"])).drop('updated_price')\n",
    "\n",
    "det_df.createOrReplaceTempView('det_df')\n",
    "display(det_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_loc(x):\n",
    "    toOut = re.findall('[+,-]*\\d+\\.\\d+',x)\n",
    "    if len(toOut) == 0:\n",
    "        return [None,None]\n",
    "    else:\n",
    "        return [float(x) for x in toOut]\n",
    "\n",
    "loc_udf = functions.udf(lambda x: find_loc(x), ArrayType(FloatType()))\n",
    "det_loc_df = det_df.withColumn('location', loc_udf(det_df.location)).orderBy('attraction_id')\n",
    "det_loc_df = det_loc_df.withColumn('latitude',det_loc_df.location[0]).withColumn('longitude',det_loc_df.location[1]).drop('location')\n",
    "\n",
    "display(det_loc_df.count())\n",
    "det_loc_df.createOrReplaceTempView('det_loc_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_loc_df.createOrReplaceTempView('det_loc_df')\n",
    "avg_cc_loc = spark.sql(\"SELECT city, category, AVG(latitude) as cc_lat, AVG(longitude) as cc_lon FROM det_loc_df WHERE ISNULL(latitude) = false AND ISNULL(longitude) = false GROUP BY city, category\")\n",
    "\n",
    "det_avgloc_df = det_loc_df.join(avg_cc_loc, ['city','category'],'left_outer')\n",
    "det_avgloc_df = det_avgloc_df.withColumn('latitude', functions.when(det_avgloc_df['latitude'].isNull(),det_avgloc_df['cc_lat']).otherwise(det_avgloc_df['latitude'])).withColumn('longitude', functions.when(det_avgloc_df['longitude'].isNull(),det_avgloc_df['cc_lon']).otherwise(det_avgloc_df['longitude'])).drop(det_avgloc_df['cc_lat']).drop(det_avgloc_df['cc_lon']).orderBy('attraction_id')\n",
    "\n",
    "display(det_avgloc_df.count())\n",
    "display(det_avgloc_df.toPandas().head(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your google maps api key below\n",
    "def get_loc(address, position, maps_key='----'):\n",
    "    maps_api_url = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "    request_url = maps_api_url + '?' + urllib.parse.urlencode({'address':address,'key':maps_key})\n",
    "    response = requests.get(request_url)\n",
    "    resp_json_payload = response.json()\n",
    "    out = resp_json_payload['results'][0]['geometry']['location']\n",
    "    if position == 'latitude':\n",
    "        return float(out['lat'])\n",
    "    elif position == 'longitude':\n",
    "        return float(out['lng'])\n",
    "\n",
    "get_lat_udf = functions.udf(lambda x: get_loc(x,'latitude'), FloatType())\n",
    "get_lon_udf = functions.udf(lambda x: get_loc(x,'longitude'), FloatType())\n",
    "det_avgloc_df = det_avgloc_df.withColumn('latitude',functions.when(det_avgloc_df['latitude'].isNull(),get_lat_udf(det_avgloc_df['city']+','+det_avgloc_df['province'])).otherwise(det_avgloc_df['latitude']))\n",
    "det_avgloc_df = det_avgloc_df.withColumn('longitude',functions.when(det_avgloc_df['longitude'].isNull(),get_lon_udf(det_avgloc_df['city']+','+det_avgloc_df['province'])).otherwise(det_avgloc_df['longitude']))\n",
    "\n",
    "display(det_avgloc_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_avgloc_df.coalesce(8).write.parquet('etl/attractions',mode='overwrite')\n",
    "det_avg_log.toPandas().to_json('etl/attractions.json',orient='records',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df = spark.read.parquet(rev_path).repartition(160)\n",
    "display(rev_df.count())\n",
    "display(rev_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(ip_date):\n",
    "    try:\n",
    "        op_date = dt.strptime(ip_date, \"%B %d, %Y\").strftime(\"%d-%m-%Y\")\n",
    "        return op_date\n",
    "    except:\n",
    "        return ip_date\n",
    "convert_df_udf = functions.udf(lambda x: convert_date(x),StringType())\n",
    "out_df = rev_df.withColumn('review_date',convert_df_udf(rev_df['review_date']))\n",
    "out_df.createOrReplaceTempView('rev_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rev_count = spark.sql(\"SELECT user, COUNT(*) as rev_count FROM rev_df GROUP BY user ORDER BY rev_count DESC\")\n",
    "user_rev_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Reviews are available for {att_no} attractions.\".format(att_no = len(rev_df.select('attraction_id').distinct().collect())))\n",
    "print( \"Matrix will be higly sparse as the maximum number of reviews provided by an user is {val}.\".format(val=user_rev_count.select('rev_count').limit(1).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_rev_count.toPandas()\n",
    "user_df['user_id'] = user_df.index\n",
    "user_rev = spark.createDataFrame(user_df)\n",
    "rev_etled = out_df.join(user_rev.drop('rev_count'),'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rev_etled.coalesce(8).write.parquet('etl/attraction_reviews', mode = 'overwrite')\n",
    "rev_etled.toPandas().to_json('etl/attraction_reviews.json',orient='records',index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
